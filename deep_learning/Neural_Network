Quiz: Coding Softmax
And now, your time to shine! Let's code the formula for the Softmax function in Python.


import numpy as np

# Write a function that takes as input a list of numbers, and returns
# the list of values given by the softmax function.
def softmax(L):
    exp_L =[ np.exp(x) for x in L]
    res = [x / sum(exp_L) for x in exp_L]
    return res

    import numpy as np

    # Write a function that takes as input two lists Y, P,
    # and returns the float corresponding to their cross-entropy.
    def cross_entropy(Y, P):
        Y = np.array(Y)
        P = np.array(P)
        return -np.sum(Y*np.log(P) + (1-Y) * np.log(1-P))
#---solution---------

# Write a function that takes as input two lists Y, P,
# and returns the float corresponding to their cross-entropy.
import numpy as np

def cross_entropy(Y, P):
    Y = np.float_(Y)
    P = np.float_(P)
    return -np.sum(Y * np.log(P) + (1 - Y) * np.log(1 - P))


#-------------------------------------
    import numpy as np

    def sigmoid(x):
        """
        Calculate sigmoid
        """
        return 1/(1+np.exp(-x))

    def sigmoid_prime(x):
        """
        # Derivative of the sigmoid function
        """
        return sigmoid(x) * (1 - sigmoid(x))

    learnrate = 0.5
    x = np.array([1, 2, 3, 4])
    y = np.array(0.5)

    # Initial weights
    w = np.array([0.5, -0.5, 0.3, 0.1])

    ### Calculate one gradient descent step for each weight
    ### Note: Some steps have been consilated, so there are
    ###       fewer variable names than in the above sample code

    # TODO: Calculate the node's linear combination of inputs and weights
    h = np.dot(x, w)

    # TODO: Calculate output of neural network
    nn_output = sigmoid(h)

    # TODO: Calculate error of neural network
    error = y - nn_output

    # TODO: Calculate the error term
    #       Remember, this requires the output gradient, which we haven't
    #       specifically added a variable for.
    error_term = error * sigmoid_prime(h)

    # TODO: Calculate change in weights
    del_w = learnrate * error_term * x

    print('Neural Network output:')
    print(nn_output)
    print('Amount of Error:')
    print(error)
    print('Change in Weights:')
    print(del_w)

  #-------------------------------------
  Implementing gradient descent
  import numpy as np
  from data_prep import features, targets, features_test, targets_test


  def sigmoid(x):
      """
      Calculate sigmoid
      """
      return 1 / (1 + np.exp(-x))

  # TODO: We haven't provided the sigmoid_prime function like we did in
  #       the previous lesson to encourage you to come up with a more
  #       efficient solution. If you need a hint, check out the comments
  #       in solution.py from the previous lecture.

  # Use to same seed to make debugging easier
  np.random.seed(42)

  n_records, n_features = features.shape
  last_loss = None

  # Initialize weights
  weights = np.random.normal(scale=1 / n_features**.5, size=n_features)

  # Neural Network hyperparameters
  epochs = 1000
  learnrate = 0.5

  for e in range(epochs):
      del_w = np.zeros(weights.shape)
      for x, y in zip(features.values, targets):
          # Loop through all records, x is the input, y is the target

          # Note: We haven't included the h variable from the previous
          #       lesson. You can add it if you want, or you can calculate
          #       the h together with the output

          # TODO: Calculate the output
          output = sigmoid(np.dot(x, weights))

          # TODO: Calculate the error
          error = y - output

          # TODO: Calculate the error term
          error_term = error * output * (1-output)

          # TODO: Calculate the change in weights for this sample
          #       and add it to the total weight change
          del_w +=  error_term * x

      # TODO: Update weights using the learning rate and the average change in weights
      weights += learnrate *del_w/n_records

      # Printing out the mean square error on the training set
      if e % (epochs / 10) == 0:
          out = sigmoid(np.dot(features, weights))
          loss = np.mean((out - targets) ** 2)
          if last_loss and last_loss < loss:
              print("Train loss: ", loss, "  WARNING - Loss Increasing")
          else:
              print("Train loss: ", loss)
          last_loss = loss

  # Calculate accuracy on test data
  tes_out = sigmoid(np.dot(features_test, weights))
  predictions = tes_out > 0.5
  accuracy = np.mean(predictions == targets_test)
  print("Prediction accuracy: {:.3f}".format(accuracy))

  #---------------------------------------------

  import numpy as np

  def sigmoid(x):
      """
      Calculate sigmoid
      """
      return 1/(1+np.exp(-x))

  # Network size
  N_input = 4
  N_hidden = 3
  N_output = 2

  np.random.seed(42)
  # Make some fake data
  X = np.random.randn(4)

  weights_input_to_hidden = np.random.normal(0, scale=0.1, size=(N_input, N_hidden))
  weights_hidden_to_output = np.random.normal(0, scale=0.1, size=(N_hidden, N_output))


  # TODO: Make a forward pass through the network

  hidden_layer_in = np.dot(X, weights_input_to_hidden)
  hidden_layer_out = sigmoid(hidden_layer_in)

  print('Hidden-layer Output:')
  print(hidden_layer_out)

  output_layer_in = np.dot(hidden_layer_out, weights_hidden_to_output)
  output_layer_out = sigmoid(output_layer_in)

  print('Output-layer Output:')
  print(output_layer_out)

  #--------------------------------------

  import numpy as np


def sigmoid(x):
    """
    Calculate sigmoid
    """
    return 1 / (1 + np.exp(-x))


x = np.array([0.5, 0.1, -0.2])
target = 0.6
learnrate = 0.5

weights_input_hidden = np.array([[0.5, -0.6],
                                 [0.1, -0.2],
                                 [0.1, 0.7]])

weights_hidden_output = np.array([0.1, -0.3])

## Forward pass
hidden_layer_input = np.dot(x, weights_input_hidden)
hidden_layer_output = sigmoid(hidden_layer_input)

output_layer_in = np.dot(hidden_layer_output, weights_hidden_output)
output = sigmoid(output_layer_in)

print('hidden_layer_input', hidden_layer_input)
print('hidden_layer_output', hidden_layer_output)
print('output_layer_in', output_layer_in)
print('output', output)
print('\n')
## Backwards pass
## TODO: Calculate output error
error = target - output


# TODO: Calculate error term for output layer
output_error_term =  error * output * (1 - output)

# TODO: Calculate error term for hidden layer
hidden_error_term = np.dot(weights_hidden_output, output_error_term) * \
                    hidden_layer_output * (1-hidden_layer_output)

print('error', error)
print('output_error_term', output_error_term)
print('sum of error term * weights', np.dot(weights_hidden_output, output_error_term))
print('hidden_error_term', hidden_error_term)
print('\n')

# TODO: Calculate change in weights for hidden layer to output layer
delta_w_h_o = learnrate * output_error_term * hidden_layer_output

# TODO: Calculate change in weights for input layer to hidden layer
delta_w_i_h = learnrate * hidden_error_term * x[:,None]



print('Change in weights for hidden layer to output layer:')
print(delta_w_h_o)
print('Change in weights for input layer to hidden layer:')
print(delta_w_i_h)

## output:
('hidden_layer_input', array([ 0.24, -0.46]))
('hidden_layer_output', array([0.55971365, 0.38698582]))
('output_layer_in', -0.06012438223148006)
('output', 0.48497343084992534)


('error', 0.11502656915007464)
('output_error_term', 0.028730669543515018)
('sum of error term * weights', array([ 0.00287307, -0.0086192 ]))
('hidden_error_term', array([ 0.00070802, -0.00204471]))


Change in weights for hidden layer to output layer:
[0.00804047 0.00555918]
Change in weights for input layer to hidden layer:
[[ 1.77005547e-04 -5.11178506e-04]
 [ 3.54011093e-05 -1.02235701e-04]
 [-7.08022187e-05  2.04471402e-04]]

Nice job!  That's right!
#------------------------------------
import numpy as np
from data_prep import features, targets, features_test, targets_test

np.random.seed(21)

def sigmoid(x):
    """
    Calculate sigmoid
    """
    return 1 / (1 + np.exp(-x))


# Hyperparameters
n_hidden = 2  # number of hidden units
epochs = 900
learnrate = 0.005

n_records, n_features = features.shape
last_loss = None
# Initialize weights
weights_input_hidden = np.random.normal(scale=1 / n_features ** .5,
                                        size=(n_features, n_hidden))
weights_hidden_output = np.random.normal(scale=1 / n_features ** .5,
                                         size=n_hidden)

for e in range(epochs):
    del_w_input_hidden = np.zeros(weights_input_hidden.shape)
    del_w_hidden_output = np.zeros(weights_hidden_output.shape)
    for x, y in zip(features.values, targets):
        ## Forward pass ##
        # TODO: Calculate the output
        hidden_input = np.dot(x, weights_input_hidden)
        hidden_output = sigmoid(hidden_input)
        output = sigmoid(np.dot(hidden_output, weights_hidden_output))

        ## Backward pass ##
        # TODO: Calculate the network's prediction error
        error = y - output

        # TODO: Calculate error term for the output unit
        output_error_term = error * output * (1-output)

        ## propagate errors to hidden layer

        # TODO: Calculate the hidden layer's contribution to the error
        hidden_error = np.dot(output_error_term, weights_hidden_output)

        # TODO: Calculate the error term for the hidden layer
        hidden_error_term = hidden_error * hidden_output * (1-hidden_output)

        # TODO: Update the change in weights
        del_w_hidden_output += output_error_term * hidden_output
        del_w_input_hidden +=  hidden_error_term * x[:,None]

    # TODO: Update weights
    weights_input_hidden += learnrate * del_w_input_hidden / n_records
    weights_hidden_output += learnrate * del_w_hidden_output / n_records

    # Printing out the mean square error on the training set
    if e % (epochs / 10) == 0:
        hidden_output = sigmoid(np.dot(x, weights_input_hidden))
        out = sigmoid(np.dot(hidden_output,
                             weights_hidden_output))
        loss = np.mean((out - targets) ** 2)

        if last_loss and last_loss < loss:
            print("Train loss: ", loss, "  WARNING - Loss Increasing")
        else:
            print("Train loss: ", loss)
        last_loss = loss

# Calculate accuracy on test data
hidden = sigmoid(np.dot(features_test, weights_input_hidden))
out = sigmoid(np.dot(hidden, weights_hidden_output))
predictions = out > 0.5
accuracy = np.mean(predictions == targets_test)
print("Prediction accuracy: {:.3f}".format(accuracy))


#----
Building a Neural Network in Keras
Here are some core concepts you need to know for working with Keras.

Sequential Model
    from keras.models import Sequential

    #Create the Sequential model
    model = Sequential()
The keras.models.Sequential class is a wrapper for the neural network model that treats the network as a sequence of layers. It implements the Keras model interface with common methods like compile(), fit(), and evaluate() that are used to train and run the model. We'll cover these functions soon, but first let's start looking at the layers of the model.

Layers
The Keras Layer class provides a common interface for a variety of standard neural network layers. There are fully connected layers, max pool layers, activation layers, and more. You can add a layer to a model using the model's add() method. For example, a simple model with a single hidden layer might look like this:

    import numpy as np
    from keras.models import Sequential
    from keras.layers.core import Dense, Activation

    # X has shape (num_rows, num_cols), where the training data are stored
    # as row vectors
    X = np.array([[0, 0], [0, 1], [1, 0], [1, 1]], dtype=np.float32)

    # y must have an output vector for each input vector
    y = np.array([[0], [0], [0], [1]], dtype=np.float32)

    # Create the Sequential model
    model = Sequential()

    # 1st Layer - Add an input layer of 32 nodes with the same input shape as
    # the training samples in X
    model.add(Dense(32, input_dim=X.shape[1]))

    # Add a softmax activation layer
    model.add(Activation('softmax'))

    # 2nd Layer - Add a fully connected output layer
    model.add(Dense(1))

    # Add a sigmoid activation layer
    model.add(Activation('sigmoid'))
Keras requires the input shape to be specified in the first layer, but it will automatically infer the shape of all other layers. This means you only have to explicitly set the input dimensions for the first layer.

The first (hidden) layer from above, model.add(Dense(32, input_dim=X.shape[1])), creates 32 nodes which each expect to receive 2-element vectors as inputs. Each layer takes the outputs from the previous layer as inputs and pipes through to the next layer. This chain of passing output to the next layer continues until the last layer, which is the output of the model. We can see that the output has dimension 1.

The activation "layers" in Keras are equivalent to specifying an activation function in the Dense layers (e.g., model.add(Dense(128)); model.add(Activation('softmax')) is computationally equivalent to model.add(Dense(128, activation="softmax")))), but it is common to explicitly separate the activation layers because it allows direct access to the outputs of each layer before the activation is applied (which is useful in some model architectures).

Once we have our model built, we need to compile it before it can be run. Compiling the Keras model calls the backend (tensorflow, theano, etc.) and binds the optimizer, loss function, and other parameters required before the model can be run on any input data. We'll specify the loss function to be categorical_crossentropy which can be used when there are only two classes, and specify adam as the optimizer (which is a reasonable default when speed is a priority). And finally, we can specify what metrics we want to evaluate the model with. Here we'll use accuracy.

model.compile(loss="categorical_crossentropy", optimizer="adam", metrics = ["accuracy"])
We can see the resulting model architecture with the following command:

model.summary()
The model is trained with the fit() method, through the following command that specifies the number of training epochs and the message level (how much information we want displayed on the screen during training).

model.fit(X, y, nb_epoch=1000, verbose=0)
Note: In Keras 1, nb_epoch sets the number of epochs, but in Keras 2 this changes to the keyword epochs.

Finally, we can use the following command to evaluate the model:

model.evaluate()
Pretty simple, right? Let's put it into practice.

import numpy as np
from keras.utils import np_utils
import tensorflow as tf
tf.python.control_flow_ops = tf
tf.logging.set_verbosity(tf.logging.ERROR)
# Set random seed
np.random.seed(42)

# Our data
X = np.array([[0,0],[0,1],[1,0],[1,1]]).astype('float32')
y = np.array([[0],[1],[1],[0]]).astype('float32')

# Initial Setup for Keras
from keras.models import Sequential
from keras.layers.core import Dense, Activation, Flatten

# One-hot encoding the output
y = np_utils.to_categorical(y)

# Building the model
xor = Sequential()
xor.add(Dense(32, input_dim=2))
xor.add(Activation("tanh"))
xor.add(Dense(32))
xor.add(Activation('relu'))
xor.add(Dense(2))
xor.add(Activation("sigmoid"))

xor.compile(loss="categorical_crossentropy", optimizer="adam", metrics = ['accuracy'])

# Uncomment this line to print the model architecture
# xor.summary()

# Fitting the model
history = xor.fit(X, y, nb_epoch=50, verbose=0)

# Scoring the model
score = xor.evaluate(X, y)
print("\nAccuracy: ", score[-1])

# Checking the predictions
print("\nPredictions:")
print(xor.predict_proba(X))

#-------------------


#-----------------
